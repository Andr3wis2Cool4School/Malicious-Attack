{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85216e0",
   "metadata": {},
   "source": [
    "**LUO YIFENG**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3e830",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "941d23f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "        \"\"\"\n",
    "        This function cleans the text in the following ways\n",
    "        1. Replace websites with URL\n",
    "        2. Replace 's with <space>'s (e.g., her's --> her 's)\n",
    "        \"\"\"\n",
    "        text = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"URL\", text) # Replace urls with special token\n",
    "        #text = text.replace(\"\\'s\", \"\")\n",
    "        #text = text.replace(\"\\'\", \"\")\n",
    "        #text = text.replace(\"n\\'t\", \" n\\'t\")\n",
    "        #text = text.replace(\"@\", \"\")\n",
    "        #text = text.replace(\"#\", \"\")\n",
    "        #text = text.replace(\"_\", \" \")\n",
    "        #text = text.replace(\"-\", \" \")\n",
    "        text = text.replace(\"&amp;\", \"\")\n",
    "        text = text.replace(\"&gt;\", \"\")\n",
    "        text = text.replace(\"\\\"\", \"\")\n",
    "        text = text.replace(\"$MENTION$\", '')\n",
    "        text = text.replace(\"$ URL $\", '')\n",
    "        text = text.replace(\"$URL$\", '')\n",
    "        #text = text.replace(\".\", \"\")\n",
    "        #text = text.replace(\",\", \"\")\n",
    "        #text = text.replace(\"(\", \"\")\n",
    "        #text = text.replace(\")\", \"\")\n",
    "        text = text.replace(\"<end>\", \"\")\n",
    "        text = ' '.join(text.split())\n",
    "        return text.strip()\n",
    "\n",
    "# word level\n",
    "def tokenize(lines):\n",
    "    return [line.split() for line in lines]\n",
    "\n",
    "def clean(data):\n",
    "    for index, text in enumerate(data):\n",
    "        text = clean_text(text)\n",
    "        data[index] = text\n",
    "    \n",
    "    return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0cec110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "X = df.Text.to_list()\n",
    "y = []\n",
    "for label in df.Label:\n",
    "    if label == 'non-rumors':\n",
    "        y.append(0)\n",
    "    elif label == 'rumors':\n",
    "        y.append(1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=8)\n",
    "\n",
    "\n",
    "X_train = clean(X_train)\n",
    "X_test = clean(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c284031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenize = tokenize(X_train)\n",
    "X_test_tokenize = tokenize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555665d2",
   "metadata": {},
   "source": [
    "### Bulid Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb4e9ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        \"\"\"Defined in :numref:`sec_text_preprocessing`\"\"\"\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # Sort according to frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):  # Index for the unknown token\n",
    "        return self._token_freqs\n",
    "    \n",
    "    \n",
    "def count_corpus(tokens):\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bcb10681",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(X_train_tokenize, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b53a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]\n",
    "    return line + [padding_token] * (num_steps - len(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "319a4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_features = torch.tensor([truncate_pad(\n",
    "        vocab[line], 500, vocab['<pad>']) for line in X_train_tokenize])\n",
    "test_features = torch.tensor([truncate_pad(\n",
    "    vocab[line], 500, vocab['<pad>']) for line in X_test_tokenize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "472d6406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   9,   0,  ...,   0,   0,   0],\n",
       "        [139,  24, 721,  ...,   0,   0,   0],\n",
       "        [  0,  28,  26,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 80, 137,  39,  ...,   0,   0,   0],\n",
       "        [  0,  87,  25,  ...,   0,   0,   0],\n",
       "        [526,   0,   0,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8952f30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BREAKING: French media reporting two suspects of #CharlieHebdo attack are killed | More at: http://t.co/jKrAfvH9sT http://t.co/oaZH2sz6fO'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a191c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "train_iter = load_array((train_features, torch.tensor(y_train)), 32)\n",
    "test_iter = load_array((test_features, torch.tensor(y_test)), 32, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c77ba",
   "metadata": {},
   "source": [
    " # Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "59c323e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BiLstm(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, num_classes):\n",
    "        super(BiLstm, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers, bidirectional=True)\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x.T)\n",
    "        self.encoder.flatten_parameters()\n",
    "        outputs, _ = self.encoder(embeddings)\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b74b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, num_classes = 100, 100, 2, 2\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\n",
    "\n",
    "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "             for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    if type(m) == nn.LSTM:\n",
    "        for param in m._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(m._parameters[param])\n",
    "                \n",
    "net = BiLstm(len(vocab), embed_size, num_hiddens, num_layers, num_classes)\n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323342bc",
   "metadata": {},
   "source": [
    "## World Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8325e7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fb6f7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HUB = dict()\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "DATA_HUB['glove.6b.100d'] = (DATA_URL + 'glove.6B.100d.zip',\n",
    "                                 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n",
    "def download(name, cache_dir=os.path.join('..', 'data')):\n",
    "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\n",
    "\n",
    "    Defined in :numref:`sec_kaggle_house`\"\"\"\n",
    "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  # Hit cache\n",
    "    print(f'Downloading {fname} from {url}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6002c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_extract(name, folder=None):\n",
    "    \"\"\"Download and extract a zip/tar file.\n",
    "\n",
    "    Defined in :numref:`sec_kaggle_house`\"\"\"\n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    if ext == '.zip':\n",
    "        fp = zipfile.ZipFile(fname, 'r')\n",
    "    elif ext in ('.tar', '.gz'):\n",
    "        fp = tarfile.open(fname, 'r')\n",
    "    else:\n",
    "        assert False, 'Only zip/tar files can be extracted.'\n",
    "    fp.extractall(base_dir)\n",
    "    return os.path.join(base_dir, folder) if folder else data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0a1e8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding:\n",
    "    \"\"\"Token Embedding.\"\"\"\n",
    "    def __init__(self, embedding_name):\n",
    "        \"\"\"Defined in :numref:`sec_synonyms`\"\"\"\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n",
    "            embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in\n",
    "                             enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        data_dir = download_extract(embedding_name)\n",
    "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText website: https://fasttext.cc/\n",
    "        with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
    "                   for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4d877092",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = TokenEmbedding('glove.6b.100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "70773602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1030, 100])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3db6e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.embedding.weight.data.copy_(embeds)\n",
    "net.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e13f8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bfcc52d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e096322",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print('Epoch: [{}/{}]'.format(epoch + 1, num_epochs))\n",
    "    for i, (X, y) in enumerate(train_iter):\n",
    "        # forward\n",
    "        y_hat = net(X)\n",
    "        # Compute the loss\n",
    "        loss = loss_function(y_hat, y)\n",
    "        \n",
    "        # clean the gradient \n",
    "        optimizer.zero_grad()\n",
    "        # do back propagation\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            total_steps = len(train_iter)\n",
    "            true = y.data.cpu\n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
