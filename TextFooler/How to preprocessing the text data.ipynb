{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06690339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNote that we can not import Counter\\nThe reason is that Counter is a function\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "'''\n",
    "Note that we can not import Counter\n",
    "The reason is that Counter is a function\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b6b8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        \"\"\"\n",
    "        This function cleans the text in the following ways\n",
    "        1. Replace websites with URL\n",
    "        2. Replace 's with <space>'s (e.g., her's --> her 's)\n",
    "        \"\"\"\n",
    "        text = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"URL\", text) # Replace urls with special token\n",
    "        #text = text.replace(\"\\'s\", \"\")\n",
    "        #text = text.replace(\"\\'\", \"\")\n",
    "        #text = text.replace(\"n\\'t\", \" n\\'t\")\n",
    "        text = text.replace(\"@\", \"\")\n",
    "        text = text.replace(\":\", \"\")\n",
    "        text = text.replace(\"#\", \"\")\n",
    "        text = text.replace(\"_\", \" \")\n",
    "        text = text.replace(\"-\", \" \")\n",
    "        text = text.replace(\"&amp;\", \"\")\n",
    "        text = text.replace(\"&gt;\", \"\")\n",
    "        text = text.replace(\"\\\"\", \"\")\n",
    "        text = text.replace(\"$MENTION$\", '')\n",
    "        text = text.replace(\"$ URL $\", '')\n",
    "        text = text.replace(\"$URL$\", '')\n",
    "        text = text.replace(\".\", \"\")\n",
    "        text = text.replace(\",\", \"\")\n",
    "        text = text.replace(\"(\", \"\")\n",
    "        text = text.replace(\")\", \"\")\n",
    "        text = text.replace(\"<end>\", \"\")\n",
    "        text = text.replace(\"|\", \"\")\n",
    "        text = text.lower()\n",
    "        return text.strip() # Return a copy of the string with leading and trailing whitespace removed.\n",
    "\n",
    "def change_label(label):\n",
    "    if label == 'non-rumors':\n",
    "        label = 0\n",
    "    elif label == 'rumors':\n",
    "        label = 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "935c3300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0       [the, appropriate, response, to, charliehebdo,...\n",
       " 1       [breaking, french, media, reporting, two, susp...\n",
       " 2       [i'm, muslim, will, forever, defend, freedom, ...\n",
       " 3       [six, explosions, heard, at, kosher, supermark...\n",
       " 4       [brilliant, charliehebdo, satirists, did, not,...\n",
       "                               ...                        \n",
       " 3231    [update, shooter, darrenwilson, didn't, know, ...\n",
       " 3232    [the, officers, were, literally, putting, hand...\n",
       " 3233    [shorter, ferguson, pd, damn, we, thought, we,...\n",
       " 3234    [report, ferguson, cop, id'd, as, unarmed, tee...\n",
       " 3235    [nbcnews, i, demand, that, you, retract, the, ...\n",
       " Name: Text, Length: 3236, dtype: object,\n",
       " 0       0\n",
       " 1       0\n",
       " 2       0\n",
       " 3       0\n",
       " 4       0\n",
       "        ..\n",
       " 3231    1\n",
       " 3232    1\n",
       " 3233    1\n",
       " 3234    1\n",
       " 3235    1\n",
       " Name: Label, Length: 3236, dtype: int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data.csv')\n",
    "X = dataset['Text']\n",
    "y = dataset['Label']\n",
    "\n",
    "X = X.apply(lambda x: clean_text(x))\n",
    "X = X.apply(lambda x: x.split())\n",
    "\n",
    "y = y.apply(lambda x: change_label(x))\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1164da9f",
   "metadata": {},
   "source": [
    "The string type of the token is inconvenient to be used by models, which take numerical inputs. Now let us build a dictionary, often called **vocabulary 词典** as well, to map string tokens into numerical indices starting from 0. To do so, we first count the unique tokens in all the documents from the training set, namely a **corpus 语料**, and then assign a numerical index to each unique token according to its frequency.\n",
    "\n",
    "Any token that does not exist in the corpus or has been removed is mapped into a special unknown token “<unk>”. We optionally add a list of reserved tokens, such as “<pad>” for padding, “<bos>” to present the beginning for a sequence, and “<eos>” for the end of a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c9a19d",
   "metadata": {},
   "source": [
    "#### The question is how to count the token frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "30ed9bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'appropriate',\n",
       " 'response',\n",
       " 'to',\n",
       " 'charliehebdo',\n",
       " 'is',\n",
       " 'not',\n",
       " 'to',\n",
       " 'urge',\n",
       " 'respect',\n",
       " 'for',\n",
       " 'islam',\n",
       " 'but',\n",
       " 'to',\n",
       " 'assert',\n",
       " 'liberty',\n",
       " 'jesuischarlie',\n",
       " 'url',\n",
       " 'breaking',\n",
       " 'french',\n",
       " 'media',\n",
       " 'reporting',\n",
       " 'two',\n",
       " 'suspects',\n",
       " 'of',\n",
       " 'charliehebdo',\n",
       " 'attack',\n",
       " 'are',\n",
       " 'killed',\n",
       " 'more',\n",
       " 'at',\n",
       " 'url',\n",
       " 'url']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for demo purpose, only take two of them and store them into the Var called demo\n",
    "demo = X[0:2]\n",
    "tokens = [token for text in demo for token in text]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "59b4b358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 1,\n",
       "         'appropriate': 1,\n",
       "         'response': 1,\n",
       "         'to': 3,\n",
       "         'charliehebdo': 2,\n",
       "         'is': 1,\n",
       "         'not': 1,\n",
       "         'urge': 1,\n",
       "         'respect': 1,\n",
       "         'for': 1,\n",
       "         'islam': 1,\n",
       "         'but': 1,\n",
       "         'assert': 1,\n",
       "         'liberty': 1,\n",
       "         'jesuischarlie': 1,\n",
       "         'url': 3,\n",
       "         'breaking': 1,\n",
       "         'french': 1,\n",
       "         'media': 1,\n",
       "         'reporting': 1,\n",
       "         'two': 1,\n",
       "         'suspects': 1,\n",
       "         'of': 1,\n",
       "         'attack': 1,\n",
       "         'are': 1,\n",
       "         'killed': 1,\n",
       "         'more': 1,\n",
       "         'at': 1})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = collections.Counter(tokens)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "827eaf38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " dict_items([('the', 1), ('appropriate', 1), ('response', 1), ('to', 3), ('charliehebdo', 2), ('is', 1), ('not', 1), ('urge', 1), ('respect', 1), ('for', 1), ('islam', 1), ('but', 1), ('assert', 1), ('liberty', 1), ('jesuischarlie', 1), ('url', 3), ('breaking', 1), ('french', 1), ('media', 1), ('reporting', 1), ('two', 1), ('suspects', 1), ('of', 1), ('attack', 1), ('are', 1), ('killed', 1), ('more', 1), ('at', 1)]),\n",
       " dict_items)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt['to'], cnt.items(), type(cnt.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d1a6dd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'appropriate', 'response', 'to', 'charliehebdo', 'is', 'not', 'to', 'urge', 'respect', 'for', 'islam', 'but', 'to', 'assert', 'liberty', 'jesuischarlie', 'url']\n",
      "['breaking', 'french', 'media', 'reporting', 'two', 'suspects', 'of', 'charliehebdo', 'attack', 'are', 'killed', 'more', 'at', 'url', 'url']\n",
      "[\"i'm\", 'muslim', 'will', 'forever', 'defend', 'freedom', 'of', 'speech', 'jesuischarlieterrorists', 'make', 'a', 'mockery', 'of', 'islam', 'url']\n",
      "['six', 'explosions', 'heard', 'at', 'kosher', 'supermarket', 'where', 'a', 'number', 'of', 'hostages', 'were', 'taken', 'parisattacks', 'url', 'url']\n",
      "['brilliant', 'charliehebdo', 'satirists', 'did', 'not', 'attack', 'islam', 'but', 'poked', 'fun', 'intolerance', 'extremism', 'of', 'all', 'kind', 'inc', 'racism', 'islamophobia']\n",
      "['islam', 'is', 'not', 'the', 'enemy', 'muslims', 'are', 'not', 'the', 'enemy', 'hatecannotdriveouthate', 'jesuischarlie']\n",
      "['charlie', 'hebdo', 'will', 'print', '1', 'million', 'copies', 'of', 'next', 'issue', 'url', 'jesuischarlie', 'url']\n",
      "['i', 'hope', 'one', 'day', 'no', 'one', 'will', 'insult', 'the', 'prophet', 'not', 'because', 'of', 'terrorist', 'threats', 'but', 'because', 'he', 'is', 'universally', 'respected', 'charliehebdo']\n",
      "['the', 'staff', 'of', 'agence', 'france', 'presse', 'stand', 'in', 'silence', 'solidarity', 'to', 'remember', 'their', 'murdered', 'colleagues', 'jesuischarlie', 'url']\n",
      "['france', 'on', 'high', 'alert', 'after', 'deadly', 'attack', 'on', 'paris', 'offices', 'of', 'charlie', 'hebdo', 'url', 'url']\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for i in X.to_list():\n",
    "    print(i)\n",
    "    n = n + 1\n",
    "    if n == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9abb94ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Therefore we define a function to count the tokens\n",
    "'''\n",
    "def count_corpus(text_X):\n",
    "    tokens = [token for text in text_X for token in text]\n",
    "    cnt = collections.Counter(tokens)\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e02b3248",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, X):\n",
    "        cnt = count_corpus(X)\n",
    "        self._token_freqs = sorted(cnt.items(), key=lambda x: x[1], reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        # a list\n",
    "        self.idx_to_token = ['<unk>']\n",
    "        # dict comperhension\n",
    "        # a dict\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        \n",
    "        for token, freq in self._token_freqs:\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "   \n",
    "    '''\n",
    "    takes an index and returns a tuple with sample data to be used for training\n",
    "    \n",
    "    **In this Case**!!!!\n",
    "    It should take one row of text and return the index of the all the token\n",
    "    If there is no such token inside the `token_to_idx`, 0 is returned\n",
    "                        \\*\n",
    "                        dict.get(key, default=None)\n",
    "                        take two parameters\n",
    "                        If key not exists, in this case, 0 is returned --> self.UNK\n",
    "                        */\n",
    "    Input should be a token list x,\n",
    "    Output should be a index list. \n",
    "    '''\n",
    "    def __getitem__(self, x):\n",
    "        if not isinstance(x, (list, tuple)):\n",
    "            return self.token_to_idx.get(x, self.unk)\n",
    "        return [self.__getitem__(token) for token in x]\n",
    "        \n",
    "    '''\n",
    "    Input the list of indices\n",
    "    Output the tokens\n",
    "    '''    \n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    \n",
    "    '''\n",
    "    Allow us to access the method just like the attribute\n",
    "    '''\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d7a1b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('url', 1), ('the', 2), ('in', 3), ('ferguson', 4), ('charliehebdo', 5), ('of', 6), ('to', 7), ('a', 8), ('police', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(X)\n",
    "print(list(vocab.token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "89ead7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['the', 'appropriate', 'response', 'to', 'charliehebdo', 'is', 'not', 'to', 'urge', 'respect', 'for', 'islam', 'but', 'to', 'assert', 'liberty', 'jesuischarlie', 'url']\n",
      "Indices: [2, 1993, 221, 7, 5, 11, 24, 7, 2903, 507, 17, 121, 52, 7, 2904, 549, 19, 1]\n",
      "Text: ['attack', 'that', 'killed', 'policewoman', 'south', 'of', 'paris', 'being', 'treated', 'as', 'terrorist', 'french', 'prosecutors', 'say', 'url']\n",
      "Indices: [15, 27, 34, 449, 1997, 6, 12, 129, 1303, 30, 109, 28, 1556, 56, 1]\n"
     ]
    }
   ],
   "source": [
    "# only the 0 and 10\n",
    "for i in [0, 10]:\n",
    "    print('Text:', X[i])\n",
    "    print('Indices:', vocab[X[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
